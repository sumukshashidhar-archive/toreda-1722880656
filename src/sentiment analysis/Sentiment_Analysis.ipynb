{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMLuSMf3c5EqFeBCPOzMHy8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumukshashidhar/toreda/blob/master/src/sentiment%20analysis/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFCGc-5JTMEb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "a70508aa-3c6b-4090-b08e-2885a93715aa"
      },
      "source": [
        "!pip install vaderSentiment\n",
        "!pip install newspaper3k\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.6/dist-packages (3.3.1)\n",
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 2.8MB/s \n",
            "\u001b[?25hCollecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/0e/9ab599d6e78f0340bb1d1e28ddeacb38c8bb7f91a1b0eae9a24e9603782f/tldextract-2.2.2-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hCollecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Collecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 11.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (46.1.3)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.3->newspaper3k) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Building wheels for collected packages: tinysegmenter, feedparser, feedfinder2, jieba3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13539 sha256=497fc39049be66e191f5549f8b5b455913945c0c91687bc0327ce438109bc138\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44940 sha256=552d07b72df69257b9e863693d8cb112d4ceae203e1703dd0bbba6c333c7b2ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3357 sha256=96cfcd004bcd22882b1c7c5b1bf3466a97d3986d7a0087da6485d9b8da700034\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=5934ac923192d3169bb36054f2f332f490ecfd18c4e07f456365eecacb45dd6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "Successfully built tinysegmenter feedparser feedfinder2 jieba3k\n",
            "Installing collected packages: tinysegmenter, cssselect, requests-file, tldextract, feedparser, feedfinder2, jieba3k, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 tinysegmenter-0.3 tldextract-2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ5VRpUjSzlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import requests\n",
        "import nltk\n",
        "import argparse\n",
        "import logging\n",
        "import string\n",
        "\n",
        "try:\n",
        "    import urllib.parse as urlparse\n",
        "except ImportError:\n",
        "    import urlparse\n",
        "from tweepy.streaming import StreamListener\n",
        "from tweepy import API, Stream, OAuthHandler, TweepError\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from random import randint, randrange\n",
        "from datetime import datetime\n",
        "from newspaper import Article, ArticleException"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqBmM7j6S6lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentimentURL = 'http://text-processing.com/api/sentiment/'\n",
        "\n",
        "\n",
        "# tweet id list\n",
        "tweet_ids = []\n",
        "\n",
        "\n",
        "prev_time = time.time()\n",
        "sentiment_avg = [0.0,0.0,0.0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISSsrh22S-Eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sentiment_from_url(text, sentimentURL):\n",
        "    # get sentiment from text processing website\n",
        "    payload = {'text': text}\n",
        "\n",
        "    try:\n",
        "        #logger.debug(text)\n",
        "        post = requests.post(sentimentURL, data=payload)\n",
        "        #logger.debug(post.status_code)\n",
        "        #logger.debug(post.text)\n",
        "    except requests.exceptions.RequestException as re:\n",
        "        logger.error(\"Exception: requests exception getting sentiment from url caused by %s\" % re)\n",
        "        raise\n",
        "\n",
        "    # return None if we are getting throttled or other connection problem\n",
        "    if post.status_code != 200:\n",
        "        logger.warning(\"Can't get sentiment from url caused by %s %s\" % (post.status_code, post.text))\n",
        "        return None\n",
        "\n",
        "    response = post.json()\n",
        "\n",
        "    neg = response['probability']['neg']\n",
        "    pos = response['probability']['pos']\n",
        "    neu = response['probability']['neutral']\n",
        "    label = response['label']\n",
        "\n",
        "    # determine if sentiment is positive, negative, or neutral\n",
        "    if label == \"neg\":\n",
        "        sentiment = \"negative\"\n",
        "    elif label == \"neutral\":\n",
        "        sentiment = \"neutral\"\n",
        "    else:\n",
        "        sentiment = \"positive\"\n",
        "\n",
        "    return sentiment, neg, pos, neu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7VTIC85TDJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a77ecfc5-1455-45ff-8f93-4c13c15f861e"
      },
      "source": [
        "get_sentiment_from_url(\"I hate you\", sentimentURL)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('negative', 0.7400333317066653, 0.2599666682933347, 0.043876766913714414)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmuI1TsYVNqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    # clean up text\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = re.sub(r\"https?\\S+\", \"\", text)\n",
        "    text = re.sub(r\"&.*?;\", \"\", text)\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "    text = text.replace(\"RT\", \"\")\n",
        "    text = text.replace(u\"…\", \"\")\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R9mBL7-VM7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text_sentiment(text):\n",
        "    # clean up text for sentiment analysis\n",
        "    text = re.sub(r\"[#|@]\\S+\", \"\", text)\n",
        "    text = text.strip()\n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsYjjeHWUFfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentiment_analysis(text):\n",
        "    \"\"\"Determine if sentiment is positive, negative, or neutral\n",
        "    algorithm to figure out if sentiment is positive, negative or neutral\n",
        "    uses sentiment polarity from TextBlob, VADER Sentiment and\n",
        "    sentiment from text-processing URL\n",
        "    \"\"\"\n",
        "\n",
        "    # pass text into sentiment url\n",
        "    if True:\n",
        "        ret = get_sentiment_from_url(text, sentimentURL)\n",
        "        if ret is None:\n",
        "            sentiment_url = None\n",
        "        else:\n",
        "            sentiment_url, neg_url, pos_url, neu_url = ret\n",
        "    else:\n",
        "        sentiment_url = None\n",
        "\n",
        "    # pass text into TextBlob\n",
        "    text_tb = TextBlob(text)\n",
        "\n",
        "    # pass text into VADER Sentiment\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    text_vs = analyzer.polarity_scores(text)\n",
        "\n",
        "    # determine sentiment from our sources\n",
        "    if sentiment_url is None:\n",
        "        #threshold values\n",
        "        if text_tb.sentiment.polarity < 0 and text_vs['compound'] <= -0.05:\n",
        "            sentiment = \"negative\"\n",
        "        elif text_tb.sentiment.polarity > 0 and text_vs['compound'] >= 0.05:\n",
        "            sentiment = \"positive\"\n",
        "        else:\n",
        "            sentiment = \"neutral\"\n",
        "    else:\n",
        "        # this works if the above function executes properly\n",
        "        if text_tb.sentiment.polarity < 0 and text_vs['compound'] <= -0.05 and sentiment_url == \"negative\":\n",
        "            sentiment = \"negative\"\n",
        "        elif text_tb.sentiment.polarity > 0 and text_vs['compound'] >= 0.05 and sentiment_url == \"positive\":\n",
        "            sentiment = \"positive\"\n",
        "        else:\n",
        "            sentiment = \"neutral\"\n",
        "\n",
        "    polarity = (text_tb.sentiment.polarity + text_vs['compound']) / 2\n",
        "\n",
        "    # output sentiment polarity\n",
        "    print(\"************\")\n",
        "    print(\"Sentiment Polarity: \" + str(round(polarity, 3)))\n",
        "\n",
        "    # output sentiment subjectivity (TextBlob)\n",
        "    print(\"Sentiment Subjectivity: \" + str(round(text_tb.sentiment.subjectivity, 3)))\n",
        "\n",
        "    # output sentiment\n",
        "    print(\"Sentiment (url): \" + str(sentiment_url))\n",
        "    print(\"Sentiment (algorithm): \" + str(sentiment))\n",
        "    print(\"Overall sentiment (textblob): \", text_tb.sentiment)\n",
        "    print(\"Overall sentiment (vader): \", text_vs)\n",
        "    print(\"sentence was rated as \", round(text_vs['neg']*100, 3), \"% Negative\")\n",
        "    print(\"sentence was rated as \", round(text_vs['neu']*100, 3), \"% Neutral\")\n",
        "    print(\"sentence was rated as \", round(text_vs['pos']*100, 3), \"% Positive\")\n",
        "    print(\"************\")\n",
        "\n",
        "    return polarity, text_tb.sentiment.subjectivity, sentiment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBw_-un_UQFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "242eb80f-60ff-4622-d03f-9f229fda6a84"
      },
      "source": [
        "sentiment_analysis(\"Sumuk Shashidhar is the best in tHIS world\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "************\n",
            "Sentiment Polarity: 0.818\n",
            "Sentiment Subjectivity: 0.3\n",
            "Sentiment (url): neutral\n",
            "Sentiment (algorithm): neutral\n",
            "Overall sentiment (textblob):  Sentiment(polarity=1.0, subjectivity=0.3)\n",
            "Overall sentiment (vader):  {'neg': 0.0, 'neu': 0.625, 'pos': 0.375, 'compound': 0.6369}\n",
            "sentence was rated as  0.0 % Negative\n",
            "sentence was rated as  62.5 % Neutral\n",
            "sentence was rated as  37.5 % Positive\n",
            "************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.81845, 0.3, 'neutral')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84wDTTBQUTXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_page_text(url):\n",
        "\n",
        "    max_paragraphs = 10\n",
        "\n",
        "    try:\n",
        "        logger.debug(url)\n",
        "        req = requests.get(url)\n",
        "        html = req.text\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        html_p = soup.findAll('p')\n",
        "\n",
        "        logger.debug(html_p)\n",
        "\n",
        "        if html_p:\n",
        "            n = 1\n",
        "            for i in html_p:\n",
        "                if n <= max_paragraphs:\n",
        "                    if i.string is not None:\n",
        "                        logger.debug(i.string)\n",
        "                        yield i.string\n",
        "                n += 1\n",
        "\n",
        "    except requests.exceptions.RequestException as re:\n",
        "        logger.warning(\"Exception: can't crawl web site (%s)\" % re)\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5eD0oc2VUUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f55e283-7ee6-4daf-b806-464583a7f21d"
      },
      "source": [
        "get_page_text('https://python.org')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object get_page_text at 0x7fae6f3b2830>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkMn5LUBVbos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}